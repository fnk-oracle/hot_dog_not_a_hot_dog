# -*- coding: utf-8 -*-
"""Hot Dog & Not Hot Dog.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1e0qI3PVpy2aSggD1WQxxHOstuXcu8z6C
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import tensorflow as tf
import random
from tensorflow.keras import layers, models, datasets
import tensorflow_datasets as tfds

ds, ds_info = tfds.load('food101', shuffle_files=True, as_supervised=True, with_info=True)

train_ds, valid_ds = ds['train'], ds['validation']

fig = tfds.show_examples(train_ds, ds_info)

MAX_SIDE_LEN = 128
HOT_DOG_CLASS = 55
train_ds = train_ds.map(
    lambda image, label: (tf.cast(tf.image.resize(image, [MAX_SIDE_LEN, MAX_SIDE_LEN]), dtype=tf.int32), tf.cast(label == HOT_DOG_CLASS, dtype=tf.int32))
)
valid_ds = valid_ds.map(
    lambda image, label: (tf.cast(tf.image.resize(image, [MAX_SIDE_LEN, MAX_SIDE_LEN]), dtype=tf.int32), tf.cast(label == HOT_DOG_CLASS, dtype=tf.int32))
)

fig = tfds.show_examples(train_ds, ds_info)

train_hd_size, valid_hd_size = 750, 250
train_hotdogs = train_ds.filter(lambda image, label: label == 1).repeat(3)
train_not_hotdogs = train_ds.filter(lambda image, label: label == 0)

valid_hotdogs = valid_ds.filter(lambda image, label: label == 1).repeat(3)
valid_not_hotdogs = valid_ds.filter(lambda image, label: label == 0)

valid_hotdogs

batch_size = 16

train_ds = tf.data.Dataset.sample_from_datasets([train_hotdogs, train_not_hotdogs], weights=[0.5, 0.5], stop_on_empty_dataset=True)
train_ds = train_ds.cache().batch(batch_size).prefetch(tf.data.AUTOTUNE)

valid_ds = tf.data.Dataset.sample_from_datasets([valid_hotdogs, valid_not_hotdogs], weights=[0.5, 0.5], stop_on_empty_dataset=True)
valid_ds = valid_ds.cache().batch(batch_size).prefetch(tf.data.AUTOTUNE)

for image, label in train_ds.take(1):
    print(image[0][0][0], label)

"""Neural Network Implementation"""

data_augmentation = tf.keras.Sequential([
    tf.keras.layers.RandomFlip('horizontal'),
    tf.keras.layers.RandomRotation(0.2),
])

for i, _ in ds["train"].take(1):
  image = i

plt.imshow(image)

image =  tf.cast(tf.expand_dims(image, 0), tf.float32)
image /= 255.0

plt.figure(figsize=(10, 10))
for i in range(9):
  augmented_image = data_augmentation(image) # Remove the extra dimension
  ax = plt.subplot(3, 3, i + 1)
  plt.imshow(augmented_image[0])
  plt.axis("off")

random.seed(0)
model = models.Sequential()
model.add(layers.Rescaling(1./255))
model.add(layers.Conv2D(128, (3, 3), activation='relu', input_shape=(MAX_SIDE_LEN, MAX_SIDE_LEN, 3)))
model.add(layers.MaxPooling2D((2, 2)))
model.add(layers.Dropout(0.25))
model.add(layers.Conv2D(64, (3, 3), activation='relu', kernel_regularizer=tf.keras.regularizers.l2(l2=0.01)))
model.add(layers.MaxPooling2D((2, 2)))
model.add(layers.Dropout(0.25))
model.add(layers.Conv2D(32, (3, 3), activation='relu', kernel_regularizer=tf.keras.regularizers.l2(l2=0.01)))
model.add(layers.Flatten())
model.add(layers.Dense(128, activation='relu'))
model.add(layers.Dropout(0.25))
model.add(layers.Dense(1))

lr = 0.0001
model.compile(optimizer=tf.keras.optimizers.Adam(lr),
              loss=tf.keras.losses.BinaryCrossentropy(from_logits=False),
              metrics=['accuracy'])

epochs=50
history = model.fit(
  train_ds,
  validation_data=valid_ds,
  epochs=epochs,
)

plt.figure(figsize=(10, 10))
for image_batch, label_batch in valid_ds.take(1):
  images = image_batch
  labels = label_batch

for i in range(9):
  ax = plt.subplot(3, 3, i+1)
  plt.imshow(images[i])
  plt.axis("off")

labels[:9]

